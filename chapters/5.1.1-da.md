数据分析
---

数据分析是一个很有意思的过程，我们可以简单地将这个过程分成四个步骤：

 - 识别需求
 - 收集数据
 - 分析数据
 - 展示数据

值得注意的是：在分析数据的过程中，需要不同的人员来参与，需要跨域多个领域的知识点——分析、设计、开发、商业和研究等领域。因此，在这样的领域里，回归敏捷也是一种不错的选择（源于：《敏捷数据科学》）：

 - 通才高于专长
 - 小团队高于大团队
 - 使用高阶工具和平台：云计算、分布式系统、PaaS
 - 持续、迭代地分享工作成果，即使这些工作未完成

###识别需求

在我们开始分析数据之前，我们需要明确一下，我们的问题是什么？即，我们到底要干嘛，我们想要的内容是什么。

> 识别信息需求是确保数据分析过程有效性的首要条件，可以为收集数据、分析数据提供清晰的目标。

当我们想要提到我们的网站在不同地区的速度时，就需要去探索我们的用户主要是在哪些地区。即，现在这是我们的需求。我们已经有了这样的一个明确的目标，下面要做起来就很轻松了。

###收集数据

那么现在新的问题来了，我们的数据要从哪里来？

对于大部分的网站来说，都会有访问日志。但是这些访问日志只能显示某个 IP 进入了某个页面，并不能详细地介绍这个用户在这个页面待了多久，做了什么事。这时候，就需要依赖于类似 Google Analytics 这样的工具来统计网站的流量。还有类似于New Relic这样的工具来统计用户的一些行为。

在一些以科学研究为目的的数据收集中，我们可以从一些公开的数据中获取这些资料。

而在一些特殊的情况里，我们就需要通过爬虫来完成这样的工作。

###分析数据

现在，我们终于可以真正的去分析数据了——我的意思是，我们要开始写代码了。从海量的数据中过滤出我们想要的数据，并通过算法来对其进行分析。

一般来说，我们都利用现有的工具来完成大部分的工作。要使用哪一类工具，取决于我们要分析的数据的数量级了。如果只是一般的数量级，我们可以考虑用 R 语言、Python、Octave 等单机工具来完成。如果是大量的数据，那么我们就需要考虑用 Hadoop、Spark 来完成这个级别的工作。

而一般来说，这个过程可能是要经过一系列的工具才能完成。如在之前我在分析我的博客的日志时(1G左右)，我用 Hadoop + Apache Pig + Jython 来将日志中的 IP 转换为 GEO 信息，再将 GEO 信息存储到 ElasticSearch 中。随后，我们就可以用 AMap、leaflet 这一类 GEO 库将这些点放置到地图上。

###展示数据

现在，终于来到我最喜欢的环节了，也是最有意思，但是却又最难的环节。

我们过滤数据，得到想要的内容后，就要去考虑如何可视化这些数据。在我熟悉的 Web GIS领域里，我可以可视化出我过滤后的那些数据。但是对于我不熟悉的领域，要可视化这些数据不是一件容易的事。在少数情况下，可以使用现有的工具完成需求，多数情况下，我们也需要写相当的代码才能将数据最后可视化出来。

而在以什么形式来展示我们的数据时，又是一个问题。如一般的数据结果，到底是使用柱形图、条形图、折线图还是面积图？这要求我们有一些 UX 方面的经验。

参考来源: **精益数据分析**。
